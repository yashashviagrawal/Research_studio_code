{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "yYJryUTz5N1d"
   },
   "outputs": [],
   "source": [
    "class Neo4jConnection:\n",
    "    def __init__(self, uri, user, pwd):  # Changed _init_ to __init__\n",
    "        self.__uri = uri\n",
    "        self.__user = user\n",
    "        self.__pwd = pwd\n",
    "        self.__driver = None  # Initialize __driver to None\n",
    "        try:\n",
    "            self.__driver = GraphDatabase.driver(self.__uri, auth=(self.__user, self.__pwd))  # Use self.__uri, self.__user, self.__pwd and assign to self.__driver\n",
    "        except Exception as e:\n",
    "            print(\"Failed to create the driver:\", e)\n",
    "\n",
    "    def close(self):\n",
    "        if self.__driver is not None:\n",
    "            self.__driver.close()\n",
    "\n",
    "    def query(self, query, parameters=None, db=None):\n",
    "        assert self.__driver is not None, \"Driver not initialized!\"\n",
    "        session = None\n",
    "        response = None\n",
    "        try:\n",
    "            session = self.__driver.session(database=db) if db is not None else self.__driver.session() # Use self.__driver\n",
    "            response = list(session.run(query, parameters))\n",
    "        except Exception as e:\n",
    "            print(\"Query failed:\", e)\n",
    "        finally:\n",
    "            if session is not None:\n",
    "                session.close()\n",
    "        return response\n",
    "\n",
    "    def get_driver(self):\n",
    "        return self.__driver #Return self.__driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "vbjJnzIG5R3b"
   },
   "outputs": [],
   "source": [
    "def populate_neo4j(conn, data):\n",
    "    # Access the driver using the getter method\n",
    "    with conn.get_driver().session() as session:  # Use a transaction\n",
    "        for item in tqdm(data, total=len(data)):  # Iterate through the list of dictionaries\n",
    "            wall_id = item[\"wall_id\"]\n",
    "            materials = item[\"materials\"]\n",
    "\n",
    "            # Create Wall node\n",
    "            session.run(\"\"\"\n",
    "                MERGE (w:Wall {id: $wall_id})\n",
    "            \"\"\", parameters={'wall_id': wall_id})\n",
    "\n",
    "            # Create Material nodes and relationships\n",
    "            for material in materials:\n",
    "                session.run(\"\"\"\n",
    "                    MERGE (m:Material {name: $name})\n",
    "                    ON CREATE SET m.density = $density, m.conductivity = $conductivity,\n",
    "                                 m.u_value = $u_value, m.embodied_carbon_coefficient = $carbon_coeff,\n",
    "                                 m.cost = $cost, m.recyclability = $recyclability,\n",
    "                                 m.bio_based = $bio_based, m.color = $color\n",
    "                    ON MATCH SET m.density = $density, m.conductivity = $conductivity,\n",
    "                                 m.u_value = $u_value, m.embodied_carbon_coefficient = $carbon_coeff,\n",
    "                                 m.cost = $cost, m.recyclability = $recyclability,\n",
    "                                 m.bio_based = $bio_based, m.color = $color\n",
    "                    MERGE (w:Wall {id: $wall_id})\n",
    "                    MERGE (m)-[:USED_IN {thickness: $thickness}]->(w)\n",
    "                \"\"\", parameters={\n",
    "                    'name': material['material'],\n",
    "                    'wall_id': wall_id,\n",
    "                    'thickness': material['thickness'],\n",
    "                    'density': material['density'],\n",
    "                    'conductivity': material['conductivity'],\n",
    "                    'u_value': material['u_value'],\n",
    "                    'carbon_coeff': material['embodied_carbon_coefficient'],\n",
    "                    'cost': material['cost'],\n",
    "                    'recyclability': material['recyclability'],\n",
    "                    'bio_based': material['bio_based'],\n",
    "                    'color': material['color'],\n",
    "                })\n",
    "\n",
    "            # Create Metric nodes for additional properties and relationships\n",
    "            metrics = {\n",
    "                'construction_demolition_waste': item.get('construction_demolition_waste'),\n",
    "                'circular_economy': item.get('circular_economy'),\n",
    "                'heritage_preservation': item.get('heritage_preservation'),\n",
    "                'responsible_material_sourcing': item.get('responsible_material_sourcing'),\n",
    "                'embodied_ghg_emissions': item.get('embodied_ghg_emissions'),\n",
    "                'affordable_adoption_high_quality_housing_conditions': item.get('affordable_adoption_high_quality_housing_conditions')\n",
    "            }\n",
    "\n",
    "            # Loop over each metric and create nodes and relationships\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                if metric_value is not None:\n",
    "                    session.run(\"\"\"\n",
    "                        MERGE (m:Metric {name: $metric_name})\n",
    "                        ON CREATE SET m.value = $value\n",
    "                        ON MATCH SET m.value = $value\n",
    "                        MERGE (w:Wall {id: $wall_id})\n",
    "                        MERGE (w)-[:HAS_METRIC {value: $value}]->(m)\n",
    "                    \"\"\", parameters={\n",
    "                        'metric_name': metric_name,\n",
    "                        'value': metric_value,\n",
    "                        'wall_id': wall_id\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HVJGliov5Uug",
    "outputId": "a503135d-bf02-4c15-c8ac-e62890fbc116"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Connecting to Neo4j...\n",
      "Populating the Neo4j database...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:50<00:00, 39.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neo4j database population complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to your JSON file\n",
    "    file_path = r'D:\\Research\\wall_population.json'  # Use raw string for Windows paths\n",
    "\n",
    "    # Load JSON data\n",
    "    print(\"Loading data...\")\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Neo4j connection details\n",
    "    uri = \"bolt://localhost:7687\"  # Replace with your Neo4j URI\n",
    "    user = \"neo4j\"                 # Replace with your Neo4j username\n",
    "    password = \"123456789\"         # Replace with your Neo4j password\n",
    "\n",
    "    # Establish connection\n",
    "    print(\"Connecting to Neo4j...\")\n",
    "    conn = Neo4jConnection(uri, user, password)\n",
    "\n",
    "    # Populate Neo4j database\n",
    "    print(\"Populating the Neo4j database...\")\n",
    "    try:\n",
    "        populate_neo4j(conn, data)\n",
    "    finally:\n",
    "        # Close connection\n",
    "        conn.close()\n",
    "        print(\"Neo4j database population complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node properties:\n",
      "Wall {id: INTEGER}\n",
      "Material {name: STRING, density: FLOAT, conductivity: FLOAT, u_value: FLOAT, embodied_carbon_coefficient: FLOAT, cost: FLOAT, recyclability: INTEGER, bio_based: BOOLEAN, color: STRING}\n",
      "Metric {name: STRING, value: FLOAT}\n",
      "Relationship properties:\n",
      "USED_IN {thickness: FLOAT}\n",
      "HAS_METRIC {value: FLOAT}\n",
      "The relationships:\n",
      "(:Wall)-[:HAS_METRIC]->(:Metric)\n",
      "(:Material)-[:USED_IN]->(:Wall)\n"
     ]
    }
   ],
   "source": [
    "graph.refresh_schema()\n",
    "print(graph.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'count(n)': 2023}]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cypher = \"\"\"\n",
    "  MATCH (n) \n",
    "  RETURN count(n)\n",
    "  \"\"\"\n",
    "result = graph.query(cypher)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'numberOfMaterial': 18}]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cypher = \"\"\"\n",
    "  MATCH  (m:Material) \n",
    "  RETURN count(m) AS numberOfMaterial \n",
    "  \"\"\"\n",
    "graph.query(cypher)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cypher = \"\"\"\n",
    "  MATCH (rockwool:Material {name:\"rockwool\"}) \n",
    "  RETURN rockwool\n",
    "  \"\"\"\n",
    "graph.query(cypher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'metric_name': 'construction_demolition_waste', 'metric_value': 68.46}, {'metric_name': 'circular_economy', 'metric_value': 91.29}, {'metric_name': 'heritage_preservation', 'metric_value': 0.0}, {'metric_name': 'responsible_material_sourcing', 'metric_value': 100.0}, {'metric_name': 'embodied_ghg_emissions', 'metric_value': 93.69}]\n"
     ]
    }
   ],
   "source": [
    "cypher_query = \"\"\"\n",
    "MATCH (w:Wall {id: $wall_id})-[:HAS_METRIC]->(m:Metric)\n",
    "RETURN m.name AS metric_name, m.value AS metric_value\n",
    "\"\"\"\n",
    "\n",
    "parameters = {'wall_id': 1010}  # Replace with the actual wall ID\n",
    "results = graph.query(cypher_query, parameters)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['natural_query', 'cypher_query'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['natural_query', 'cypher_query'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Load your custom JSON dataset\n",
    "data = [\n",
    "    {\"natural_query\": \"Find all nodes connected to node A.\", \"cypher_query\": \"MATCH (a {name: 'A'})--(b) RETURN b;\"},\n",
    "    {\"natural_query\": \"Get the shortest path between node A and node B.\", \"cypher_query\": \"MATCH p=shortestPath((a {name: 'A'})-[*]-(b {name: 'B'})) RETURN p;\"}\n",
    "]\n",
    "\n",
    "dataset = Dataset.from_list(data)\n",
    "\n",
    "# Split into train and validation sets\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_one = \"What are the materials in wall 1010?\"\n",
    "q_two = \"What is the cricluar economic score for wall 1010?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response for Q1: \n",
      "Expert: Ah, an excellent question! *adjusts glasses* Wall 1010, you say? Let me just check our lovely graph... *consults query results*\n",
      "\n",
      "Well, it looks like Wall 1010 is made up of a variety of materials. *scrolls through results* We have wood, concrete, steel, and even some interesting blends of materials. *glances around* It's quite the mix!\n",
      "\n",
      "Would you like me to drill down further into any of these materials? Perhaps you're interested in knowing more about the types of wood used? Or maybe you want to see which companies produced the concrete and steel? Just let me know, my friend! *smiles*\n",
      "\n",
      "Response for Q2: Expert: Ah, a query about Cypher! *adjusts glasses* The circular economic score for wall 1010? Let me see... *checks graph database* Hmm, I'm afraid that information isn't stored in the graph directly. However, I can help you find out by querying the relevant data sources.\n",
      "\n",
      "May I ask what you need to know this score for? Are you working on a project related to sustainability or environmental impact? *curious*\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.tools import Tool # Make sure to import Tool \n",
    "# Import the create_react_agent and AgentExecutor functions \n",
    "from langchain.agents import AgentExecutor, create_react_agent,AgentOutputParser  # Make sure you import the hub object for pulling prompts \n",
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_community.chat_message_histories import Neo4jChatMessageHistory\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from uuid import uuid4\n",
    "import streamlit as st\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = 'True'\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = str(os.getenv(\"LANGCHAIN_API_KEY\"))\n",
    "\n",
    "# Neo4j Connection Class\n",
    "class Neo4jConnection:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self._driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        if self._driver:\n",
    "            self._driver.close()\n",
    "\n",
    "    def query(self, query, parameters=None):\n",
    "        with self._driver.session() as session:\n",
    "            return session.run(query, parameters).data()\n",
    "# Define the prompt template for conversation\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a Neo4j expert having a conversation about Cypher queries\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Set up Neo4j Graph connection\n",
    "graph = Neo4jGraph(\n",
    "    url=\"bolt://localhost:7687\",\n",
    "    username=\"neo4j\",\n",
    "    password=\"123456789\"\n",
    ")\n",
    "\n",
    "# Questions\n",
    "q_one = \"What are the materials in wall 1010?\"\n",
    "q_two = \"What is the circular economic score for wall 1010?\"\n",
    "\n",
    "# Ollama LLaMA2 LLM\n",
    "llm = Ollama(model=\"llama2\")\n",
    "\n",
    "# Create Cypher chat function using StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "cypher_chat = prompt | llm | output_parser\n",
    "\n",
    "# Streamlit input handling (if used with Streamlit)\n",
    "if input_text:  # Assuming `input_text` is a user input from Streamlit\n",
    "    st.write(cypher_chat.invoke({\"input\": input_text}))  # Ensure \"input\" matches the template\n",
    "\n",
    "# Response for q_one\n",
    "response_one = cypher_chat.invoke({\"input\": q_one})\n",
    "response_two = cypher_chat.invoke({\"input\": q_two})\n",
    "\n",
    "# Print results\n",
    "print(\"Response for Q1:\", response_one)\n",
    "\n",
    "print(\"\\nResponse for Q2:\", response_two)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BaseMode' from 'pydantic' (c:\\Users\\Lenovo\\.conda\\envs\\First_env\\Lib\\site-packages\\pydantic\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraphs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Neo4jGraph\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseMode\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Ollama\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Load JSON data\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'BaseMode' from 'pydantic' (c:\\Users\\Lenovo\\.conda\\envs\\First_env\\Lib\\site-packages\\pydantic\\__init__.py)"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_10176\\3304141224.py:15: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"llama2\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from langchain.chains.openai_functions import create_structured_output_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "# Enable LangChain tracing and set API key\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = 'True'\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = str(os.getenv(\"LANGCHAIN_API_KEY\"))\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = Ollama(model=\"llama2\")\n",
    "\n",
    "# Define entities specific to this dataset\n",
    "class WallEntities(BaseModel):\n",
    "    \"\"\"Identifying information about walls and materials.\"\"\"\n",
    "    materials: List[str] = Field(\n",
    "        ..., description=\"List of material names mentioned in the text.\"\n",
    "    )\n",
    "    wall_ids: List[int] = Field(\n",
    "        ..., description=\"List of wall IDs mentioned in the text.\"\n",
    "    )\n",
    "    properties: List[str] = Field(\n",
    "        ..., description=\"List of properties (e.g., recyclability, embodied carbon) mentioned.\"\n",
    "    )\n",
    "\n",
    "class WallDataPipeline:\n",
    "    def __init__(self, llm, data):\n",
    "        self.llm = llm\n",
    "        self.data = data  # JSON data loaded into memory\n",
    "\n",
    "    # Step 1: Entity extraction\n",
    "    def prepare_entity_chain(self):\n",
    "        entity_chain_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", \"You extract materials, wall IDs, and properties from user queries.\"),\n",
    "                (\"human\", \"Extract entities from this query: {query}\"),\n",
    "            ]\n",
    "        )\n",
    "        entity_chain = create_structured_output_chain(WallEntities, self.llm, entity_chain_prompt)\n",
    "        return entity_chain\n",
    "\n",
    "    # Step 2: Map entities to the dataset\n",
    "    def map_to_data(self, entities: WallEntities):\n",
    "        \"\"\"Map extracted entities to the JSON dataset.\"\"\"\n",
    "        results = []\n",
    "\n",
    "        # Find walls by ID\n",
    "        for wall_id in entities.wall_ids:\n",
    "            wall = next((w for w in self.data if w[\"wall_id\"] == wall_id), None)\n",
    "            if wall:\n",
    "                results.append({\"wall_id\": wall_id, \"data\": wall})\n",
    "\n",
    "        # Find materials\n",
    "        for material in entities.materials:\n",
    "            for wall in self.data:\n",
    "                for mat in wall[\"materials\"]:\n",
    "                    if material.lower() in mat[\"material\"].lower():\n",
    "                        results.append({\"wall_id\": wall[\"wall_id\"], \"material\": mat})\n",
    "\n",
    "        # Find walls by properties\n",
    "        for property_name in entities.properties:\n",
    "            for wall in self.data:\n",
    "                if property_name in wall:\n",
    "                    results.append({\"wall_id\": wall[\"wall_id\"], \"property\": {property_name: wall[property_name]}})\n",
    "\n",
    "        return results\n",
    "\n",
    "    # Step 3: Generate responses\n",
    "    def prepare_response_prompt(self):\n",
    "        response_template = \"\"\"Based on the question, generate a response using the matched data:\n",
    "        Question: {query}\n",
    "        Matched Data: {data}\"\"\"\n",
    "        response_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", \"Generate a natural language response.\"),\n",
    "                (\"human\", response_template),\n",
    "            ]\n",
    "        )\n",
    "        return response_prompt\n",
    "\n",
    "    def prepare_pipeline(self):\n",
    "        entity_chain = self.prepare_entity_chain()\n",
    "        response_prompt = self.prepare_response_prompt()\n",
    "\n",
    "        pipeline = (\n",
    "            RunnablePassthrough.assign(entities=entity_chain)\n",
    "            | RunnablePassthrough.assign(matched_data=lambda x: self.map_to_data(x[\"entities\"]))\n",
    "            | response_prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        return pipeline\n",
    "\n",
    "    def run_pipeline(self, query):\n",
    "        entity_chain = self.prepare_entity_chain()\n",
    "        entities = entity_chain.run({\"query\": query})\n",
    "        matched_data = self.map_to_data(entities)\n",
    "        response_prompt = self.prepare_response_prompt()\n",
    "        response = response_prompt.run({\"query\": query, \"data\": matched_data})\n",
    "        return response\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "First_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
